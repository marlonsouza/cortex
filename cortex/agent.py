"""Cortex Agent - Main agent implementation for project scaffolding"""

import os
import asyncio
import logging
import aiohttp
import json
import pathlib
from typing import Dict, Optional, Any

from cortex.utils import clean_path, clean_content

# Configure enhanced logging
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s: %(message)s",
    handlers=[logging.StreamHandler()],
)


# Load environment variables from .env file if it exists
def load_env_file(file_path: str = ".env") -> Dict[str, str]:
    """Load environment variables from file"""
    env_vars = {}

    env_path = pathlib.Path(file_path)
    if not env_path.exists():
        logging.info(f"{file_path} not found, using defaults and environment variables")
        return env_vars

    try:
        with open(env_path, "r") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue

                try:
                    key, value = line.split("=", 1)
                    env_vars[key.strip()] = value.strip()
                except ValueError:
                    # Skip lines that don't have the expected format
                    logging.warning(f"Skipping invalid line in .env file: {line}")

        logging.info(f"Loaded configuration from {file_path}")
        return env_vars
    except Exception as e:
        logging.warning(f"Error loading {file_path}: {str(e)}")
        return env_vars


# Load .env file first
env_vars = load_env_file()


# Get value from env vars, then environment, then default
def get_config(key, default, env_vars=None):
    if env_vars and key in env_vars:
        return env_vars[key]
    return os.environ.get(key, default)


# Model Context Protocol (MCP) Configuration
MCP_API_URL = get_config("MCP_API_URL", "http://localhost:11434", env_vars)
MCP_API_KEY = get_config("MCP_API_KEY", "", env_vars)  # Set your API key if needed
MCP_MODEL = get_config("MCP_MODEL", "llama3", env_vars)
MCP_TEMPERATURE = float(get_config("MCP_TEMPERATURE", "0.7", env_vars))
MCP_MAX_TOKENS = int(get_config("MCP_MAX_TOKENS", "4096", env_vars))
MCP_TIMEOUT = int(get_config("MCP_TIMEOUT", "120", env_vars))

# Context7 Specific Configuration
CONTEXT7_ENABLED = get_config("CONTEXT7_ENABLED", "false", env_vars).lower() in [
    "true",
    "1",
    "yes",
    "y",
]
CONTEXT7_PORT = get_config("CONTEXT7_PORT", "8123", env_vars)
CONTEXT7_HOST = get_config("CONTEXT7_HOST", "localhost", env_vars)
CONTEXT7_STARTUP_TIMEOUT = int(get_config("CONTEXT7_STARTUP_TIMEOUT", "30", env_vars))
CONTEXT7_MIN_TOKENS = int(get_config("CONTEXT7_MIN_TOKENS", "10000", env_vars))
CONTEXT7_LIBS = get_config(
    "CONTEXT7_LIBS", "", env_vars
)  # Comma-separated list of libraries

# Debug mode configuration
DEBUG = get_config("DEBUG", "false", env_vars).lower() in ["true", "1", "yes", "y"]

# Start Context7 MCP server if enabled
if CONTEXT7_ENABLED:
    import subprocess
    import time
    import signal
    import atexit

    # Function to start Context7 MCP server
    def start_context7_server():
        global context7_process
        try:
            print("Starting Context7 MCP server...")
            # Use npx to start the server
            cmd = ["npx", "-y", "@upstash/context7-mcp@latest"]

            # Set environment variables for Context7
            env = os.environ.copy()
            env["PORT"] = CONTEXT7_PORT
            env["DEFAULT_MINIMUM_TOKENS"] = str(CONTEXT7_MIN_TOKENS)

            # Start the process
            context7_process = subprocess.Popen(
                cmd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
            )

            # Register cleanup function
            atexit.register(stop_context7_server)

            # Wait for server to start
            start_time = time.time()
            server_url = f"http://{CONTEXT7_HOST}:{CONTEXT7_PORT}"

            print(f"Waiting for Context7 server to start at {server_url}...")
            while time.time() - start_time < CONTEXT7_STARTUP_TIMEOUT:
                try:
                    # Try to connect to server
                    import urllib.request

                    urllib.request.urlopen(server_url, timeout=1)
                    print("Context7 MCP server started successfully")
                    return True
                except:
                    time.sleep(1)

            print("Timed out waiting for Context7 server to start")
            stop_context7_server()
            return False
        except Exception as e:
            print(f"Error starting Context7 server: {str(e)}")
            return False

    # Function to stop Context7 MCP server
    def stop_context7_server():
        global context7_process
        if "context7_process" in globals() and context7_process:
            print("Stopping Context7 MCP server...")
            try:
                context7_process.terminate()
                context7_process.wait(timeout=5)
            except:
                context7_process.kill()
            context7_process = None

    # Start the server
    if start_context7_server():
        # Update MCP URL to point to Context7
        MCP_API_URL = f"http://{CONTEXT7_HOST}:{CONTEXT7_PORT}"
    else:
        print(
            "Warning: Failed to start Context7 server, falling back to direct LLM connection"
        )
        CONTEXT7_ENABLED = False

# Show configuration info
print(f"Model Configuration:")
print(f"  Model: {MCP_MODEL}")
print(f"  Temperature: {MCP_TEMPERATURE}")
print(f"  Max Tokens: {MCP_MAX_TOKENS}")

if CONTEXT7_ENABLED:
    print(f"\nContext7 MCP Enabled:")
    print(f"  Host: {CONTEXT7_HOST}")
    print(f"  Port: {CONTEXT7_PORT}")
    print(f"  Token Limit: {CONTEXT7_MIN_TOKENS}")
    if CONTEXT7_LIBS:
        print(f"  Libraries: {CONTEXT7_LIBS}")

    # Update MCP URL to point to Context7
    MCP_API_URL = f"http://{CONTEXT7_HOST}:{CONTEXT7_PORT}"
else:
    print(f"\nDirect LLM Connection:")
    print(f"  API URL: {MCP_API_URL}")
    print(f"  API Key: {'Configured' if MCP_API_KEY else 'Not configured'}")


# MCP client class
class MCPClient:
    def __init__(
        self,
        model,
        api_url=MCP_API_URL,
        api_key=MCP_API_KEY,
        temperature=MCP_TEMPERATURE,
        max_tokens=MCP_MAX_TOKENS,
        timeout=MCP_TIMEOUT,
    ):
        self.model = model
        self.api_url = api_url
        self.api_key = api_key
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        self.headers = {"Content-Type": "application/json"}
        if api_key:
            self.headers["Authorization"] = f"Bearer {api_key}"
        self.using_context7 = CONTEXT7_ENABLED
        self.context7_libs = []
        if self.using_context7 and CONTEXT7_LIBS:
            self.context7_libs = [
                lib.strip() for lib in CONTEXT7_LIBS.split(",") if lib.strip()
            ]
            print(f"Context7 libraries: {', '.join(self.context7_libs)}")

    async def create(self, messages):
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            formatted_messages = []
            for msg in messages:
                if hasattr(msg, "role"):
                    role = msg.role
                else:
                    role = (
                        "system"
                        if getattr(msg, "__class__", None)
                        and msg.__class__.__name__ == "SystemMessage"
                        else "user"
                    )
                formatted_messages.append({"role": role, "content": msg.content})
            self.is_ollama = "ollama" in self.api_url.lower() or "11434" in self.api_url
            if self.is_ollama:
                payload = {
                    "model": self.model,
                    "messages": formatted_messages,
                    "options": {
                        "temperature": self.temperature,
                        "num_predict": self.max_tokens,
                    },
                    "stream": False,
                }
            else:
                payload = {
                    "model": self.model,
                    "messages": formatted_messages,
                    "temperature": self.temperature,
                    "max_tokens": self.max_tokens,
                    "stream": False,
                }
            if self.is_ollama:
                base_url = self.api_url.rstrip("/")
                if "11434" in base_url:
                    if "/api" in base_url:
                        if base_url.endswith("/api"):
                            endpoint = f"{base_url}/chat"
                        else:
                            endpoint = f"{base_url.split('/api')[0]}/api/chat"
                    else:
                        endpoint = f"{base_url}/api/chat"
                else:
                    if "/api/chat" in base_url:
                        endpoint = base_url
                    elif "/api" in base_url:
                        endpoint = f"{base_url.rstrip('/api')}/api/chat"
                    else:
                        endpoint = f"{base_url}/api/chat"
                if "localhost" in base_url or "127.0.0.1" in base_url:
                    try:
                        url_parts = (
                            base_url.split("://")[1] if "://" in base_url else base_url
                        )
                        host_part = url_parts.split("/")[0]
                        self.backup_endpoint = f"http://{host_part}/api/chat"
                    except Exception:
                        self.backup_endpoint = "http://localhost:11434/api/chat"
            else:
                base_url = self.api_url.rstrip("/")
                if "/v1" in base_url:
                    if base_url.endswith("/v1"):
                        endpoint = f"{base_url}/chat/completions"
                    else:
                        parts = base_url.split("/v1")
                        endpoint = f"{parts[0]}/v1/chat/completions"
                else:
                    endpoint = f"{base_url}/v1/chat/completions"
            logging.info(f"Sending request to MCP: {endpoint}")
            logging.info(
                f"Debug - API URL: {self.api_url}, Is Ollama: {self.is_ollama}"
            )
            logging.info(f"Debug - Payload: {json.dumps(payload, indent=2)}")
            max_retries = 2
            retry_count = 0
            error_messages = []
            endpoints_to_try = [endpoint]
            if self.is_ollama and hasattr(self, "backup_endpoint"):
                endpoints_to_try.append(self.backup_endpoint)
            if self.is_ollama and "localhost" in endpoint:
                endpoints_to_try.append("http://localhost:11434/api/chat")
            logging.info(f"Will try the following endpoints: {endpoints_to_try}")
            for current_endpoint in endpoints_to_try:
                retry_count = 0
                while retry_count < max_retries:
                    try:
                        logging.info(
                            f"Attempt {retry_count+1} with endpoint: {current_endpoint}"
                        )
                        async with session.post(
                            current_endpoint,
                            json=payload,
                            headers=self.headers,
                            timeout=aiohttp.ClientTimeout(total=self.timeout),
                        ) as response:
                            if response.status == 200:
                                result = await response.json()
                                logging.info(
                                    f"‚úÖ Success! Received response ({len(str(result))} chars) from: {current_endpoint}"
                                )
                                self.last_successful_endpoint = current_endpoint
                                break
                            else:
                                error_text = await response.text()
                                error_msg = f"API Error: HTTP {response.status} from {current_endpoint} - {error_text}"
                                logging.warning(error_msg)
                                error_messages.append(error_msg)
                                retry_count += 1
                    except aiohttp.ClientError as e:
                        error_msg = (
                            f"Connection error with {current_endpoint}: {str(e)}"
                        )
                        logging.warning(error_msg)
                        error_messages.append(error_msg)
                        retry_count += 1
                    except asyncio.TimeoutError:
                        error_msg = f"Timeout connecting to {current_endpoint} after {self.timeout}s"
                        logging.warning(error_msg)
                        error_messages.append(error_msg)
                        retry_count += 1
                    except Exception as e:
                        error_msg = (
                            f"Unexpected error with {current_endpoint}: {str(e)}"
                        )
                        logging.warning(error_msg)
                        error_messages.append(error_msg)
                        retry_count += 1
                if "result" in locals():
                    break
            if "result" not in locals():
                all_errors = "\n".join(error_messages)
                error_msg = f"All API connection attempts failed after trying {len(endpoints_to_try)} endpoints.\nErrors:\n{all_errors}"
                logging.error(error_msg)
                raise Exception(f"Failed to connect to LLM API: {error_msg}")
            logging.info(
                f"Received response from API with {len(str(result))} characters"
            )

            class MCPResponse:
                def __init__(self, content):
                    self.content = content

            content = ""
            if self.is_ollama:
                content = result.get("message", {}).get("content", "")
                if not content:
                    if "response" in result:
                        content = result.get("response", "")
            else:
                content = (
                    result.get("choices", [{}])[0].get("message", {}).get("content", "")
                )
            if not content:
                logging.warning("Empty content received from API")
                logging.warning(f"Response structure: {json.dumps(result, indent=2)}")
            else:
                logging.info(f"Successfully extracted content (length: {len(content)})")
            return MCPResponse(content)


# Create MCP client
client = MCPClient(model=MCP_MODEL)

if __name__ == "__main__":
    import json
    import argparse

    # Set up command line argument parser
    parser = argparse.ArgumentParser(
        description="Cortex - AI assistant to scaffold projects using MCP"
    )
    parser.add_argument(
        "--model", type=str, help=f"Model to use (default: {MCP_MODEL})"
    )
    parser.add_argument("--url", type=str, help=f"MCP API URL (default: {MCP_API_URL})")
    parser.add_argument("--api-key", type=str, help="MCP API key")
    parser.add_argument(
        "--temperature", type=float, help=f"Temperature (default: {MCP_TEMPERATURE})"
    )
    parser.add_argument(
        "--max-tokens", type=int, help=f"Max tokens (default: {MCP_MAX_TOKENS})"
    )
    parser.add_argument("--project", type=str, help="Project name (skips the prompt)")
    parser.add_argument(
        "--prompt-file",
        type=str,
        help="File containing the prompt (skips the prompt input)",
    )
    parser.add_argument("--debug", action="store_true", help="Enable debug mode")

    args = parser.parse_args()

    # Update configuration from command line args
    if args.model:
        MCP_MODEL = args.model
        print(f"Using model: {MCP_MODEL}")

    if args.url:
        MCP_API_URL = args.url
        print(f"Using MCP API URL: {MCP_API_URL}")

    if args.api_key:
        MCP_API_KEY = args.api_key
        print("API key provided via command line")

    if args.temperature is not None:
        MCP_TEMPERATURE = args.temperature
        print(f"Using temperature: {MCP_TEMPERATURE}")

    if args.max_tokens is not None:
        MCP_MAX_TOKENS = args.max_tokens
        print(f"Using max tokens: {MCP_MAX_TOKENS}")

    if args.debug:
        DEBUG = True
        print("Debug mode enabled")

    # Recreate client with updated parameters
    client = MCPClient(
        model=MCP_MODEL,
        api_url=MCP_API_URL,
        api_key=MCP_API_KEY,
        temperature=MCP_TEMPERATURE,
        max_tokens=MCP_MAX_TOKENS,
    )

    # Define message classes compatible with MCP
    class SystemMessage:
        def __init__(self, content):
            self.content = content
            self.__class__.__name__ = "SystemMessage"

    class UserMessage:
        def __init__(self, content, source=None):
            self.content = content
            self.source = source
            self.__class__.__name__ = "UserMessage"

    # Get project name
    if args.project:
        project_name = args.project
        print(f"Using project name: {project_name}")
    else:
        project_name = input("Enter new project name: ")

    os.makedirs(project_name, exist_ok=True)
    os.chdir(project_name)

    # Get prompt from file or stdin
    if args.prompt_file:
        try:
            with open(args.prompt_file, "r") as f:
                prompt = f.read()
            print(f"Read prompt from file: {args.prompt_file}")
        except Exception as e:
            print(f"Error reading prompt file: {str(e)}")
            exit(1)
    else:
        # Ask for prompt (multiline). Finish input with EOF (Ctrl-D on Unix, Ctrl-Z+Enter on Windows).
        print(
            "Enter your prompt for the assistant. Finish with EOF (Ctrl-D on Unix, Ctrl-Z+Enter on Windows):"
        )
        import sys

        prompt = sys.stdin.read()

    if not prompt.strip():
        print("Error: Empty prompt received")
        exit(1)

    print(f"Prompt length: {len(prompt)} characters")

    # Prepare messages for LLM with enhanced instruction
    system_msg = SystemMessage(
        content="You are an AI assistant that generates code and project structures based on user requirements. "
        "IMPORTANT: You must ALWAYS reply with a valid JSON object. DO NOT provide explanations or any text outside the JSON object.\n\n"
        "The JSON object must have this EXACT structure:\n"
        "{\n"
        '  "tool_calls": [\n'
        '    {"name": "make_directory", "arguments": {"path": "directory_name"}},\n'
        '    {"name": "write_file", "arguments": {"path": "file_path", "content": "file content here"}}\n'
        "  ]\n"
        "}\n\n"
        "Rules:\n"
        "1. Generate the actual code based on the user's requirements\n"
        "2. Do not include any explanations or markdown in the response\n"
        "3. The content field should contain the actual code to be written\n"
        "4. Use proper file extensions and directory structure\n"
        "5. Include all necessary imports and dependencies\n"
        "6. Follow best practices for the target language\n"
        "7. Make sure the code is complete and runnable\n"
        "8. Do not include any placeholder comments or TODOs\n"
        "9. Do not include any example code or pseudocode\n"
        "10. Generate the actual implementation that matches the user's requirements exactly\n"
    )

    # Create user message for LLM
    user_msg = UserMessage(content=prompt, source="user")

    # Call the LLM asynchronously
    import asyncio

    async def _run_plan():
        try:
            logging.info("üí¨ Sending request to language model...")
            res = await client.create([system_msg, user_msg])  # type: ignore
            logging.info("‚úÖ Successfully received response from language model")
            return res.content  # type: ignore
        except Exception as e:
            logging.error(f"‚ùå Error communicating with language model: {str(e)}")
            logging.error(
                "Please check your model configuration and network connection"
            )
            if DEBUG:
                import traceback

                logging.error("Detailed error traceback:")
                traceback.print_exc()
            raise

    try:
        print("\nüöÄ Processing your request. This may take a moment...\n")
        text = asyncio.run(_run_plan())
        print("\n‚úÖ Successfully processed your request!\n")
    except Exception as e:
        print(f"\n‚ùå Error: {str(e)}")
        print(
            "\nPlease try again with different settings or check the logs for details.\n"
        )
        if not DEBUG:
            print("Set DEBUG=true in .env file for more detailed error information.")
        exit(1)
    # DEBUG is now set from configuration

    def debug_print(*args, **kwargs):
        if DEBUG:
            print("[DEBUG]", *args, **kwargs)

    # Import modules needed for parsing
    import re

    # Attempt to load JSON; if assistant included extra text or markdown, extract JSON object
    def extract_json(s: str) -> str:
        s = s.strip()
        # find first '{' and last '}' to isolate JSON
        start = s.find("{")
        end = s.rfind("}")
        if start != -1 and end != -1 and end > start:
            debug_print(f"Extracting JSON from position {start} to {end+1}")
            return s[start : end + 1]
        debug_print("Could not find valid JSON markers, returning original string")
        return s

    def fix_escaping(json_str: str) -> str:
        # Fix common escaping issues in JSON strings from LLMs
        import re
        import json

        original = json_str

        debug_print("Original JSON string length:", len(json_str))

        if "]" not in json_str and json_str.count("{") > json_str.count("}"):
            json_str += "}]}"
            debug_print("Added missing brackets at the end")

        if "{{" in json_str or "}}" in json_str:
            json_str = json_str.replace("{{", "{").replace("}}", "}")
            debug_print("Fixed nested braces in code")

        if "HandleFunc=" in json_str:
            json_str = json_str.replace("HandleFunc=", "HandleFunc(")
            debug_print("Fixed HandleFunc syntax")

        # Fix improperly escaped quotes in JSON
        # This is a common issue with LLM-generated JSON
        try:
            # First attempt to find and fix content field with improper escaping
            content_match = re.search(
                r'"content"\s*:\s*"(.*?)(?:"\s*}|\s*$)', json_str, re.DOTALL
            )
            if content_match:
                content = content_match.group(1)
                debug_print("Found content field with length:", len(content))

                # Fix escaped quotes and newlines
                content = content.replace('\\"', '"').replace("\\n", "\n")

                # Fix Go-specific issues
                if ".go" in json_str:
                    # Fix import statements
                    content = re.sub(
                        r'import\s*\(\\"([^"]+)\\"\)', r'import ("\1")', content
                    )
                    # Fix function calls
                    content = re.sub(r'(\w+)\(\\"([^"]+)\\"\)', r'\1("\2")', content)
                    # Fix map declarations
                    content = re.sub(
                        r"map\[string\]string\{([^}]+)\}",
                        r"map[string]string{\1}",
                        content,
                    )

                # Replace the content in the JSON
                json_str = re.sub(
                    r'"content"\s*:\s*"(.*?)(?:"\s*}|\s*$)',
                    lambda m: m.group(0).replace(m.group(1), content),
                    json_str,
                    flags=re.DOTALL,
                )
                debug_print("Fixed content field escaping")
        except Exception as e:
            debug_print(f"Error while trying to fix content field: {str(e)}")

        # Fix common string escaping issues in content
        if "content" in json_str:
            # Handle newline escaping for JSON
            if "\\\\n" in json_str:
                # Already double escaped, which is correct for JSON
                pass
            elif "\\n" in json_str:
                # Need to re-escape for JSON
                json_str = json_str.replace("\\n", "\\\\n")
                debug_print("Re-escaped newlines for JSON")

        # Fix unbalanced quotes in content strings
        content_match = re.search(
            r'"content"\s*:\s*"(.*?)(?:"\s*}|\s*$)', json_str, re.DOTALL
        )
        if content_match:
            content = content_match.group(1)
            debug_print("Found content field with length:", len(content))

            # Count backslashes before quotes to identify escaped quotes
            count_escaped = content.count('\\"')
            count_unescaped = content.count('"') - count_escaped

            debug_print(
                f"Quote counts: escaped={count_escaped}, unescaped={count_unescaped}"
            )

            # If we have unbalanced quotes, try to fix them
            if count_unescaped % 2 != 0:
                debug_print("Detected unbalanced quotes, attempting to fix")
                # Find the last quote that's not escaped and add closing quotes as needed
                last_idx = json_str.rfind('"', 0, json_str.rfind("}"))
                if last_idx > 0:
                    json_str = json_str[:last_idx] + '"' + json_str[last_idx:]
                    debug_print(f"Added missing quote at position {last_idx}")

        # Check if we've made any changes
        if json_str != original:
            debug_print("JSON string was modified for parsing")

        return json_str

    debug_print(f"Raw LLM response length: {len(text)}")
    debug_print(f"Raw LLM response first 100 chars: {text[:100]}...")

    raw = text

    # Print the raw LLM response for debugging
    print("\nüìù Raw response from LLM:")
    print("-" * 40)
    print(text[:500] + "..." if len(text) > 500 else text)
    print("-" * 40 + "\n")

    # Initialize variables
    plan = None
    snippet = None
    fixed_snippet = None
    json_error = None

    # First attempt: Try parsing the raw response directly
    try:
        debug_print("Attempt 1: Parsing raw response as JSON")
        plan = json.loads(raw)
        debug_print("‚úÖ Successfully parsed raw response as JSON")
    except json.JSONDecodeError as e1:
        debug_print(f"‚ùå Failed to parse raw response: {e1}")
        json_error = e1

        # Second attempt: Try extracting what looks like JSON
        try:
            debug_print("Attempt 2: Extracting and parsing JSON-like snippet")
            snippet = extract_json(raw)
            debug_print(f"Extracted JSON-like snippet, length: {len(snippet)}")

            plan = json.loads(snippet)
            debug_print("‚úÖ Successfully parsed extracted JSON snippet")
        except json.JSONDecodeError as e2:
            debug_print(f"‚ùå Failed to parse extracted snippet: {e2}")

            # Third attempt: Apply fixes to common LLM formatting issues
            try:
                debug_print("Attempt 3: Applying fixes to JSON")
                fixed_snippet = fix_escaping(snippet or raw)
                debug_print(f"Fixed snippet length: {len(fixed_snippet)}")
                debug_print(
                    "Fixed JSON snippet (first 100 chars):",
                    (
                        fixed_snippet[:100] + "..."
                        if len(fixed_snippet) > 100
                        else fixed_snippet
                    ),
                )

                plan = json.loads(fixed_snippet)
                debug_print("‚úÖ Successfully parsed fixed JSON snippet")
            except (json.JSONDecodeError, TypeError) as e3:
                debug_print(f"‚ùå Failed to parse fixed snippet: {e3}")
                json_error = e3

    # If all JSON parsing attempts failed, try to manually extract tool calls
    if plan is None:
        print("‚ö†Ô∏è All JSON parsing attempts failed. Trying alternative approaches...")

        # First try to extract tool calls from JSON-like text
        try:
            import re

            print(
                "Approach 1: Attempting manual extraction of tool calls from JSON-like text..."
            )
            # Extract make_directory call
            dir_match = re.search(
                r'"name":\s*"make_directory".*?"path":\s*"([^"]+)"', text, re.DOTALL
            )
            # Extract write_file call
            file_match = re.search(
                r'"name":\s*"write_file".*?"path":\s*"([^"]+)"', text, re.DOTALL
            )
            # Extract content (this is tricky)
            content_match = re.search(
                r'"content":\s*"(.+?)(?:"\s*\}\s*\}|"\s*\})', text, re.DOTALL
            )

            if dir_match and file_match and content_match:
                dir_path = dir_match.group(1)
                file_path = file_match.group(1)
                content = content_match.group(1)

                # Unescape content appropriately
                content = content.replace("\\n", "\n").replace('\\"', '"')

                # Create a plan object manually
                plan = {
                    "tool_calls": [
                        {"name": "make_directory", "arguments": {"path": dir_path}},
                        {
                            "name": "write_file",
                            "arguments": {"path": file_path, "content": content},
                        },
                    ]
                }
                print(
                    "‚ö†Ô∏è JSON parsing issues detected, but proceeding with extracted tool calls."
                )

            # If the above doesn't work, try to detect and convert code examples
            elif "```" in text or "package main" in text:
                # Remove the convert_code_to_tool_calls function and its usage
                # Remove the hardcoded Go code
                # Keep the JSON parsing and tool execution logic

                # As a last resort for Go REST APIs, create a generic plan
                # Keep the JSON parsing and tool execution logic
                plan = {
                    "tool_calls": [
                        {"name": "make_directory", "arguments": {"path": "rest-api"}},
                        {
                            "name": "write_file",
                            "arguments": {
                                "path": "rest-api/main.go",
                                "content": "// This is a placeholder for the Go code",
                            },
                        },
                        {
                            "name": "read_file",
                            "arguments": {"path": "rest-api/main.go"},
                        },
                        {
                            "name": "delete_file",
                            "arguments": {"path": "rest-api/main.go"},
                        },
                    ]
                }
                print("‚ö†Ô∏è Created generic Go REST API as fallback.")

            else:
                print("=" * 80)
                print(f"Failed to parse JSON from assistant. Error: {json_error}")
                print("Could not extract tool calls from the response.")
                print("=" * 80)
                exit(1)

        except Exception as ex:
            print("=" * 80)
            print(f"Failed to parse JSON from assistant. Error: {json_error}")
            print(f"Additional error during extraction: {ex}")
            print("=" * 80)
            exit(1)

    # Make sure we have a valid tool_calls list
    tool_calls = plan.get("tool_calls", [])
    if not tool_calls and isinstance(plan, dict):
        # Try to find tool calls in the response if they're not at the top level
        for key, value in plan.items():
            if (
                isinstance(value, list)
                and len(value) > 0
                and isinstance(value[0], dict)
            ):
                if "name" in value[0] and "arguments" in value[0]:
                    tool_calls = value
                    break

    # Print minimal info about tool calls
    debug_print(f"Processing {len(tool_calls)} tool calls")

    # Process each tool call
    for call in tool_calls:
        try:
            name = call.get("name")
            args = call.get("arguments", {})

            if not name:
                debug_print(f"Skipping tool call with no name: {call}")
                continue

            debug_print(f"Executing tool: {name}")

            if name == "make_directory":
                path = args.get("path")
                if not path:
                    print("Error: No path provided for make_directory")
                    continue

                # Clean the path
                path = clean_path(path)

                full = os.path.abspath(path)
                os.makedirs(path, exist_ok=True)
                print(f"‚úì Created directory: {path}")

            elif name == "write_file":
                path = args.get("path")
                content = args.get("content", "")

                if not path:
                    print("Error: No path provided for write_file")
                    continue

                # Clean the path
                path = clean_path(path)

                # Create parent directories if they don't exist
                dir_name = os.path.dirname(path)
                if dir_name:  # Only create dirs if not empty (root directory)
                    os.makedirs(dir_name, exist_ok=True)

                # Clean the content
                content = clean_content(content)

                # Write the file
                try:
                    with open(path, "w") as f:
                        f.write(content)
                    print(f"‚úì Created file: {path}")
                except Exception as e:
                    print(f"‚úó Error creating file {path}: {str(e)}")

            elif name == "read_file":
                path = args.get("path")

                if not path:
                    print("Error: No path provided for read_file")
                    continue

                # Clean the path
                path = clean_path(path)

                full = os.path.abspath(path)
                print(f"üìÑ Reading file: {path}")
                try:
                    with open(path, "r") as f:
                        print(f.read())
                except FileNotFoundError:
                    print(f"‚úó File not found: {path}")
                except Exception as e:
                    print(f"‚úó Error reading file: {str(e)}")

            elif name == "delete_file":
                path = args.get("path")

                if not path:
                    print("Error: No path provided for delete_file")
                    continue

                # Clean the path
                path = clean_path(path)

                full = os.path.abspath(path)
                try:
                    os.remove(path)
                    print(f"üóëÔ∏è Deleted file: {path}")
                except FileNotFoundError:
                    print(f"‚úó File not found for deletion: {path}")
                except Exception as e:
                    print(f"‚úó Error deleting file: {str(e)}")

            else:
                print(f"Unknown tool: {name}")

        except Exception as e:
            print(f"Error processing tool call: {str(e)}")
            print(f"Tool call was: {call}")
